{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8134280,"sourceType":"datasetVersion","datasetId":4808241}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-19T22:01:01.997359Z","iopub.execute_input":"2024-04-19T22:01:01.997916Z","iopub.status.idle":"2024-04-19T22:01:02.011001Z","shell.execute_reply.started":"2024-04-19T22:01:01.997882Z","shell.execute_reply":"2024-04-19T22:01:02.009701Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/electricity-theft-detection/visualization.csv\n/kaggle/input/electricity-theft-detection/Validation_SansFLAG.csv\n/kaggle/input/electricity-theft-detection/Validation.csv\n/kaggle/input/electricity-theft-detection/preprocessedR95.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, confusion_matrix, \\\n    precision_recall_fscore_support, roc_auc_score\nfrom tensorflow.keras.optimizers import Adam\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Dense\n\n# Définition des constantes\ntf.random.set_seed(1234)\nepochs_number = 2\ntest_set_size = 0.2\noversampling_flag = 1\noversampling_percentage = 0.2\n\n# Fonction pour lire les données\ndef read_data():\n    rawData = pd.read_csv('S:\\Mes documents\\Bureau\\PFE MID\\Les articles\\ElectricityTheftDetection\\SmartGridFraudDetection-master\\SmartGridFraudDetection-master\\data\\preprocessedR90.csv')\n\n    # Séparation des features et de la cible\n    y = rawData[['FLAG']]\n    X = rawData.drop(['FLAG', 'CONS_NO'], axis=1)\n\n    # Réindexation des colonnes selon les dates\n    X.columns = pd.to_datetime(X.columns)\n    X = X.reindex(X.columns, axis=1)\n\n    # Division des données en ensembles d'entraînement et de test\n    X_train, X_test, y_train, y_test = train_test_split(X, y['FLAG'], test_size=test_set_size, random_state=0)\n\n    # Suréchantillonnage de la classe minoritaire pour gérer le déséquilibre\n    if oversampling_flag == 1:\n        over = SMOTE(sampling_strategy=oversampling_percentage, random_state=0)\n        X_train, y_train = over.fit_resample(X_train, y_train)\n\n    return X_train, X_test, y_train, y_test\n\n# Fonction pour afficher les résultats\ndef results(y_test, prediction):\n    print(\"Accuracy:\", 100 * accuracy_score(y_test, prediction))\n    print(\"RMSE:\", mean_squared_error(y_test, prediction, squared=False))\n    print(\"MAE:\", mean_absolute_error(y_test, prediction))\n    print(\"F1:\", 100 * precision_recall_fscore_support(y_test, prediction)[2])\n    print(\"AUC:\", 100 * roc_auc_score(y_test, prediction))\n    print(\"Matrice de confusion:\\n\", confusion_matrix(y_test, prediction), \"\\n\")\n\n# Fonction pour créer et entraîner le modèle\ndef build_and_train_model(X_train, X_test, y_train, y_test):\n    print('Building and training model:')\n\n    # Prétraitement des données pour les séquences temporelles\n    X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n    X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n    # Création du modèle\n    input_shape = (X_train.shape[1], X_train.shape[2])\n    nb_classes = 1  # Binary classification\n\n    input_layer = tf.keras.layers.Input(input_shape)\n    lstm = tf.keras.layers.LSTM(8)(input_layer)\n    lstm = tf.keras.layers.Dropout(0.2)(lstm)\n\n    permute = tf.keras.layers.Permute((2, 1))(input_layer)\n    conv1 = tf.keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(permute)\n    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n    conv1 = tf.keras.layers.Activation(activation='relu')(conv1)\n\n    conv2 = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n    conv2 = tf.keras.layers.Activation('relu')(conv2)\n\n    conv3 = tf.keras.layers.Conv1D(128, kernel_size=3, padding='same')(conv2)\n    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n    conv3 = tf.keras.layers.Activation('relu')(conv3)\n\n    gap_layer = tf.keras.layers.GlobalAveragePooling1D()(conv3)\n\n    concat = tf.keras.layers.concatenate([lstm, gap_layer])\n\n    output_layer = tf.keras.layers.Dense(nb_classes, activation='sigmoid')(concat)\n\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n\n    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    # Entraînement du modèle\n    history = model.fit(X_train, y_train, epochs=epochs_number, validation_split=0.1, shuffle=True, verbose=1)\n\n    # Prédiction et évaluation\n    predictions_proba = model.predict(X_test)\n    predictions = (predictions_proba > 0.5).astype(int)\n    model.summary()\n    results(y_test, predictions)\n\n    # Plotting\n    plt.figure(figsize=(20, 8))\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.title('Training Loss and Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss and Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n# ----Main----\nX_train, X_test, y_train, y_test = read_data()\nbuild_and_train_model(X_train, X_test, y_train, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T22:12:30.680180Z","iopub.execute_input":"2024-04-19T22:12:30.680899Z","iopub.status.idle":"2024-04-19T22:49:44.012971Z","shell.execute_reply.started":"2024-04-19T22:12:30.680865Z","shell.execute_reply":"2024-04-19T22:49:44.011465Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/82820464.py:43: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print('Consommateurs normaux:                    ', y[y['FLAG'] == 0].count()[0])\n/tmp/ipykernel_33/82820464.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print('Consommateurs avec fraude:                ', y[y['FLAG'] == 1].count()[0])\n/tmp/ipykernel_33/82820464.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"Classification en supposant aucune fraude: %.2f\" % (y[y['FLAG'] == 0].count()[0] / y.shape[0] * 100), \"%\")\n","output_type":"stream"},{"name":"stdout","text":"Consommateurs normaux:                     35671\nConsommateurs avec fraude:                 2573\nTotal des consommateurs:                   38244\nClassification en supposant aucune fraude: 93.27 %\nEnsemble de test en supposant aucune fraude: 93.24 %\n\nCNN-LSTM Model:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/150\n\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 398ms/step - accuracy: 0.9265 - loss: nan - val_accuracy: 0.9346 - val_loss: nan\nEpoch 2/150\n\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 399ms/step - accuracy: 0.9302 - loss: nan - val_accuracy: 0.9346 - val_loss: nan\nEpoch 3/150\n\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 403ms/step - accuracy: 0.9302 - loss: nan - val_accuracy: 0.9346 - val_loss: nan\nEpoch 4/150\n\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 401ms/step - accuracy: 0.9302 - loss: nan - val_accuracy: 0.9346 - val_loss: nan\nEpoch 5/150\n\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 402ms/step - accuracy: 0.9302 - loss: nan - val_accuracy: 0.9346 - val_loss: nan\nEpoch 6/150\n\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 396ms/step - accuracy: 0.9302 - loss: nan - val_accuracy: 0.9346 - val_loss: nan\nEpoch 7/150\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# ----Main----\u001b[39;00m\n\u001b[1;32m    109\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m read_data()\n\u001b[0;32m--> 110\u001b[0m \u001b[43mCNN_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[6], line 100\u001b[0m, in \u001b[0;36mCNN_LSTM\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Prédiction et évaluation\u001b[39;00m\n\u001b[1;32m    103\u001b[0m predictions_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:324\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m--> 324\u001b[0m         \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    326\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    327\u001b[0m             step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    328\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks/callback_list.py:98\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m     96\u001b[0m         callback\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch, logs)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, confusion_matrix, \\\n    precision_recall_fscore_support, roc_auc_score\nfrom tensorflow.keras.optimizers import Adam\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Dense\n\n# Définition des constantes\ntf.random.set_seed(1234)\nepochs_number = 150\ntest_set_size = 0.2\noversampling_flag = 1\noversampling_percentage = 0.2\n\n# Fonction pour lire les données\ndef read_data():\n    rawData = pd.read_csv('S:\\Mes documents\\Bureau\\PFE MID\\Les articles\\ElectricityTheftDetection\\SmartGridFraudDetection-master\\SmartGridFraudDetection-master\\data\\preprocessedR90.csv')\n\n    # Séparation des features et de la cible\n    y = rawData[['FLAG']]\n    X = rawData.drop(['FLAG', 'CONS_NO'], axis=1)\n\n    # Réindexation des colonnes selon les dates\n    X.columns = pd.to_datetime(X.columns)\n    X = X.reindex(X.columns, axis=1)\n\n    # Division des données en ensembles d'entraînement et de test\n    X_train, X_test, y_train, y_test = train_test_split(X, y['FLAG'], test_size=test_set_size, random_state=0)\n\n    # Suréchantillonnage de la classe minoritaire pour gérer le déséquilibre\n    if oversampling_flag == 1:\n        over = SMOTE(sampling_strategy=oversampling_percentage, random_state=0)\n        X_train, y_train = over.fit_resample(X_train, y_train)\n\n    return X_train, X_test, y_train, y_test\n\n# Fonction pour afficher les résultats\ndef results(y_test, prediction):\n    print(\"Accuracy:\", 100 * accuracy_score(y_test, prediction))\n    print(\"RMSE:\", mean_squared_error(y_test, prediction, squared=False))\n    print(\"MAE:\", mean_absolute_error(y_test, prediction))\n    print(\"F1:\", 100 * precision_recall_fscore_support(y_test, prediction)[2])\n    print(\"AUC:\", 100 * roc_auc_score(y_test, prediction))\n    print(\"Matrice de confusion:\\n\", confusion_matrix(y_test, prediction), \"\\n\")\n\n# Fonction pour créer et entraîner le modèle\ndef build_and_train_model(X_train, X_test, y_train, y_test):\n    print('Building and training model:')\n\n    # Prétraitement des données pour les séquences temporelles\n    X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n    X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n    # Création du modèle\n    input_shape = (X_train.shape[1], X_train.shape[2])\n    nb_classes = 1  # Binary classification\n\n    input_layer = tf.keras.layers.Input(input_shape)\n    lstm = tf.keras.layers.LSTM(8)(input_layer)\n    lstm = tf.keras.layers.Dropout(0.2)(lstm)\n\n    permute = tf.keras.layers.Permute((2, 1))(input_layer)\n    conv1 = tf.keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(permute)\n    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n    conv1 = tf.keras.layers.Activation(activation='relu')(conv1)\n\n    conv2 = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n    conv2 = tf.keras.layers.Activation('relu')(conv2)\n\n    conv3 = tf.keras.layers.Conv1D(128, kernel_size=3, padding='same')(conv2)\n    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n    conv3 = tf.keras.layers.Activation('relu')(conv3)\n\n    gap_layer = tf.keras.layers.GlobalAveragePooling1D()(conv3)\n\n    concat = tf.keras.layers.concatenate([lstm, gap_layer])\n\n    output_layer = tf.keras.layers.Dense(nb_classes, activation='sigmoid')(concat)\n\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n\n    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    # Entraînement du modèle\n    history = model.fit(X_train, y_train, epochs=epochs_number, validation_split=0.1, shuffle=True, verbose=1)\n\n    # Prédiction et évaluation\n    predictions_proba = model.predict(X_test)\n    predictions = (predictions_proba > 0.5).astype(int)\n    model.summary()\n    results(y_test, predictions)\n    \n    # Plotting\n    plt.figure(figsize=(20, 8))\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Training and Validation Loss and Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss and Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n# ----Main----\nX_train, X_test, y_train, y_test = read_data()\nbuild_and_train_model(X_train, X_test, y_train, y_test)\n","metadata":{},"execution_count":null,"outputs":[]}]}